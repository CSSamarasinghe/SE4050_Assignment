{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 3499094,
          "sourceType": "datasetVersion",
          "datasetId": 2078107
        }
      ],
      "dockerImageVersionId": 30191,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "🛒 5 class Classification Amazon Reviews 42% [CNN]",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSSamarasinghe/SE4050_Assignment/blob/IT21222740/%F0%9F%9B%92_5_class_Classification_Amazon_Reviews_48_%5BGRU%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5 classes fine-grained Text Classification Amazon Reviews  GRU**\n",
        "\n",
        "\n",
        "1. Loading Dataset (Amazon Reviews)\n",
        "2. Download and Imports\n",
        "3. Splitting it into training and testing sets.\n",
        "4. Preprocessing the data (tokenizing and padding).\n",
        "5. Building and training the model.\n",
        "6. DTesting the model on the test dataset.\n"
      ],
      "metadata": {
        "id": "-HHgImu7Jmz3"
      }
    },
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-reviews-for-sentianalysis-finegrained-csv:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2078107%2F3499094%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T150806Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0384286882e57ed5979a453d3b36e2d5ac7c5683c21364a514272ce697b7a7430176301d285f6982230aa12ded3cf24f63136365e5939a1b4cf6a627f90b21b558772ac2b852ceeda8c9115147d5d45101662dbded2dc256b64c8607b23b8a2e138946585e223ab27c2e6ced66542d64b390b7a29fb5f28667574314b9b5d6a1e728981f43b957023ed099b2adfc9609101a051eb013f254b576ffd5c986f20ab81845f69f45dcde2f9c5e05f1e3fc15e3aed90b0112a274ad37e14053eb8e6d8635f649c8362c1478bc8b587d074f6474c488f679d16695f5ed6a20ddea00f6dc13ab4b1746b75013f78e36b46727d42ca74c2ba6b6d35bd214cd27c5751ebb'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaotbsCdJmz3",
        "outputId": "268a687a-6ae7-4b86-f32b-98404121fef7"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading amazon-reviews-for-sentianalysis-finegrained-csv, 654512809 bytes compressed\n",
            "[==================================================] 654512809 bytes downloaded\n",
            "Downloaded and uncompressed: amazon-reviews-for-sentianalysis-finegrained-csv\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Download and Imports"
      ],
      "metadata": {
        "id": "SQ-1bbxOJmz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import GRU, Dense, Embedding, GlobalMaxPool1D\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-09-15T13:52:05.019808Z",
          "iopub.execute_input": "2023-09-15T13:52:05.020488Z",
          "iopub.status.idle": "2023-09-15T13:52:45.60198Z",
          "shell.execute_reply.started": "2023-09-15T13:52:05.020389Z",
          "shell.execute_reply": "2023-09-15T13:52:45.601041Z"
        },
        "trusted": true,
        "id": "HRQYKlQyJmz4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load and Preprocess the Data\n",
        "# Load the dataset\n",
        "data = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/train.csv')\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T10:31:08.695686Z",
          "iopub.execute_input": "2024-09-29T10:31:08.69668Z",
          "iopub.status.idle": "2024-09-29T10:31:27.76188Z",
          "shell.execute_reply.started": "2024-09-29T10:31:08.69663Z",
          "shell.execute_reply": "2024-09-29T10:31:27.760792Z"
        },
        "trusted": true,
        "id": "8z57rzHDJmz4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce the dataset to a manageable size (e.g., 25,000 samples)\n",
        "data_sample = data.sample(n=100000, random_state=42)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T10:32:01.160834Z",
          "iopub.execute_input": "2024-09-29T10:32:01.161197Z",
          "iopub.status.idle": "2024-09-29T10:33:43.823975Z",
          "shell.execute_reply.started": "2024-09-29T10:32:01.161155Z",
          "shell.execute_reply": "2024-09-29T10:33:43.822992Z"
        },
        "trusted": true,
        "id": "I5iTMmD3Jmz4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Extract Features and Labels\n",
        "X = data_sample['review_text'].values  # Features (text data)\n",
        "y = data_sample['class_index'].values - 1  # Labels, adjusted to start from 0 (for class indexing)\n"
      ],
      "metadata": {
        "id": "KxxOZEMSJuiz"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Split the Dataset into Train and Test Sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "EzAn_K8sJxFI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Preprocess the Text Data (Tokenization and Padding)\n",
        "# Tokenize the text (same for both train and test)\n",
        "tokenizer = Tokenizer(num_words=10000)  # Limit to top 10,000 words\n",
        "tokenizer.fit_on_texts(X_train)  # Fit tokenizer only on training data\n"
      ],
      "metadata": {
        "id": "QBoxnzZIJy7d"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to sequences\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n"
      ],
      "metadata": {
        "id": "TBLkJYnlJ0_Z"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a uniform length (e.g., 200)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=200)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=200)\n"
      ],
      "metadata": {
        "id": "sXILiBBNJ3AU"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build the GRU Model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=128, input_length=200))  # Embedding layer\n",
        "model.add(GRU(64, return_sequences=True))  # GRU layer\n",
        "model.add(GlobalMaxPool1D())  # Pooling layer\n",
        "model.add(Dense(5, activation='softmax'))  # Output layer with 5 classes (1-5 rating)"
      ],
      "metadata": {
        "id": "hwWIMPbGJ6WZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "cdPffHY3NqH4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the Model\n",
        "history = model.fit(X_train_padded, y_train, epochs=5, batch_size=64, validation_data=(X_test_padded, y_test))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T10:34:29.379041Z",
          "iopub.execute_input": "2024-09-29T10:34:29.379422Z",
          "iopub.status.idle": "2024-09-29T10:34:29.91766Z",
          "shell.execute_reply.started": "2024-09-29T10:34:29.379383Z",
          "shell.execute_reply": "2024-09-29T10:34:29.916744Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZD-ruc-Jmz4",
        "outputId": "ee685dbe-793e-49b7-f559-8fb5f9a594fa"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 282ms/step - accuracy: 0.3868 - loss: 1.3805 - val_accuracy: 0.5002 - val_loss: 1.1511\n",
            "Epoch 2/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m399s\u001b[0m 296ms/step - accuracy: 0.5416 - loss: 1.0655 - val_accuracy: 0.5119 - val_loss: 1.1213\n",
            "Epoch 3/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m388s\u001b[0m 300ms/step - accuracy: 0.6008 - loss: 0.9554 - val_accuracy: 0.5096 - val_loss: 1.1329\n",
            "Epoch 4/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 297ms/step - accuracy: 0.6532 - loss: 0.8531 - val_accuracy: 0.5051 - val_loss: 1.1828\n",
            "Epoch 5/5\n",
            "\u001b[1m1250/1250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 299ms/step - accuracy: 0.7080 - loss: 0.7480 - val_accuracy: 0.4882 - val_loss: 1.2516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Evaluate the Model on the Test Set\n",
        "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-29T10:15:11.43725Z",
          "iopub.execute_input": "2024-09-29T10:15:11.43798Z",
          "iopub.status.idle": "2024-09-29T10:15:11.443576Z",
          "shell.execute_reply.started": "2024-09-29T10:15:11.437937Z",
          "shell.execute_reply": "2024-09-29T10:15:11.442434Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T6052TUJmz4",
        "outputId": "500c3162-1651-4487-daab-e47ff736759f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 37ms/step - accuracy: 0.4839 - loss: 1.2555\n",
            "Test Accuracy: 0.4882\n"
          ]
        }
      ]
    }
  ]
}