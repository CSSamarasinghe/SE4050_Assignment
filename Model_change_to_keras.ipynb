{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0eep3fgR47suihUk4gUfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSSamarasinghe/SE4050_Assignment/blob/IT21263194/Model_change_to_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-reviews-for-sentianalysis-finegrained-csv:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2078107%2F3499094%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241003%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241003T164549Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D25a30a2ad799910d48f45f67793fb1d09f31560ae3773fa14870108018fa213261ae62aeb020867952a30d8094a21359c2546a6475193ac986bc2645a191d848c7223dbd5c68600aeb8f17d5a979be9750827328816b6743cf0d74ca79e5a28878f8606f4d3cd2176678d18b8a70a20bbf9be8037aa238d4b56bd1a0ba1a8d1fb4003bbd12f51fa6d438aaa8a7e8fa90a9291e62969f7df78c34dc6a71c76fb2f1c11e2964127136ec4d5554e9f1bdd7c530f8ddf987a08dd6cb91a849e4e98a2089149d22cb00ca687688c756071af4270e392fd79699c1f4a35ca16597523ca8bfe37fea11d5911465ea51d6eb17c1e3663da806763c8ef4e96e1f0a9ae9ab'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnvAoYukqQLj",
        "outputId": "2dfcf596-6194-4ee0-ca9d-50e9536fc0f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading amazon-reviews-for-sentianalysis-finegrained-csv, 654512809 bytes compressed\n",
            "[==================================================] 654512809 bytes downloaded\n",
            "Downloaded and uncompressed: amazon-reviews-for-sentianalysis-finegrained-csv\n",
            "Data source import complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load and preprocess data\n",
        "train_data = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/train.csv')\n",
        "test_data = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/test.csv')\n",
        "\n",
        "# Sample the dataset for manageable size (adjust as necessary)\n",
        "train_data = train_data.sample(n=60000, random_state=42)\n",
        "test_data = test_data.sample(n=25000, random_state=42)\n",
        "\n",
        "# Initialize the tokenizer and fit on training data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['review_text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = tokenizer.texts_to_sequences(train_data['review_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['review_text'])\n",
        "\n",
        "# Pad sequences to ensure uniform input size\n",
        "max_length = max(max(len(seq) for seq in X_train), max(len(seq) for seq in X_test))\n",
        "X_train_padded = pad_sequences(X_train, maxlen=max_length, padding='post')\n",
        "X_test_padded = pad_sequences(X_test, maxlen=max_length, padding='post')\n",
        "\n",
        "# Prepare labels (one-hot encoding)\n",
        "Y_train = train_data['class_index'].values.reshape(-1, 1)\n",
        "Y_test = test_data['class_index'].values.reshape(-1, 1)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "Y_train_onehot = encoder.fit_transform(Y_train)\n",
        "Y_test_onehot = encoder.transform(Y_test)\n",
        "\n",
        "# Split training data into training and validation sets\n",
        "X_train_padded, X_val_padded, Y_train_onehot, Y_val_onehot = train_test_split(\n",
        "    X_train_padded, Y_train_onehot, test_size=0.2, random_state=42)\n",
        "\n",
        "# Set hyperparameters\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Vocabulary size (plus padding)\n",
        "embedding_dim = 100  # Dimension of embeddings\n",
        "\n",
        "# Build the enhanced LSTM model\n",
        "model = Sequential()\n",
        "\n",
        "# Embedding Layer\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "\n",
        "# First LSTM Layer with Batch Normalization and Dropout\n",
        "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))  # Adding ReLU after LSTM\n",
        "model.add(Dropout(0.5))  # Add dropout for regularization\n",
        "\n",
        "# Second LSTM Layer with Batch Normalization and Dropout\n",
        "model.add(LSTM(128, return_sequences=True, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))  # Adding ReLU after LSTM\n",
        "model.add(Dropout(0.5))  # Add dropout for regularization\n",
        "\n",
        "# Third LSTM Layer with Batch Normalization (not returning sequences)\n",
        "model.add(LSTM(128, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))  # Adding ReLU after LSTM\n",
        "\n",
        "# Dense Layer with Dropout\n",
        "model.add(Dense(64, kernel_regularizer=l1_l2(l1=0.01, l2=0.01)))\n",
        "model.add(Activation('relu'))  # Adding ReLU before output layer\n",
        "model.add(Dropout(0.5))  # Add dropout for regularization\n",
        "\n",
        "# Output Layer with Softmax for multi-class classification (5 classes)\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0001)  # Reduce learning rate for more stable updates\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Define ReduceLROnPlateau callback to adjust learning rate based on validation loss\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=0.00001)\n",
        "\n",
        "# Train the model with validation data and callbacks\n",
        "history = model.fit(\n",
        "    X_train_padded,\n",
        "    Y_train_onehot,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_data=(X_val_padded, Y_val_onehot),\n",
        "    callbacks=[reduce_lr]\n",
        ")\n",
        "\n",
        "# Evaluate on test data and print results\n",
        "test_loss, test_accuracy = model.evaluate(X_test_padded, Y_test_onehot)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaTe2S1qVd_",
        "outputId": "8c06ada0-fd8e-4bca-ab94-28f6afa161d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1386s\u001b[0m 2s/step - accuracy: 0.2005 - loss: 75.4813 - val_accuracy: 0.2027 - val_loss: 16.1173 - learning_rate: 1.0000e-04\n",
            "Epoch 2/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 2s/step - accuracy: 0.1996 - loss: 10.1182 - val_accuracy: 0.1962 - val_loss: 3.0701 - learning_rate: 1.0000e-04\n",
            "Epoch 3/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1352s\u001b[0m 2s/step - accuracy: 0.1990 - loss: 2.4733 - val_accuracy: 0.1989 - val_loss: 1.6706 - learning_rate: 1.0000e-04\n",
            "Epoch 4/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1345s\u001b[0m 2s/step - accuracy: 0.2053 - loss: 1.6461 - val_accuracy: 0.1989 - val_loss: 1.6297 - learning_rate: 1.0000e-04\n",
            "Epoch 5/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1343s\u001b[0m 2s/step - accuracy: 0.2011 - loss: 1.6297 - val_accuracy: 0.1989 - val_loss: 1.6297 - learning_rate: 1.0000e-04\n",
            "Epoch 6/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1390s\u001b[0m 2s/step - accuracy: 0.2004 - loss: 1.6297 - val_accuracy: 0.1989 - val_loss: 1.6297 - learning_rate: 1.0000e-04\n",
            "Epoch 7/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1354s\u001b[0m 2s/step - accuracy: 0.2050 - loss: 1.6297 - val_accuracy: 0.1989 - val_loss: 1.6296 - learning_rate: 1.0000e-04\n",
            "Epoch 8/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1333s\u001b[0m 2s/step - accuracy: 0.2027 - loss: 1.6208 - val_accuracy: 0.1989 - val_loss: 1.6205 - learning_rate: 5.0000e-05\n",
            "Epoch 9/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1341s\u001b[0m 2s/step - accuracy: 0.2031 - loss: 1.6203 - val_accuracy: 0.1989 - val_loss: 1.6206 - learning_rate: 5.0000e-05\n",
            "Epoch 10/10\n",
            "\u001b[1m750/750\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1342s\u001b[0m 2s/step - accuracy: 0.2042 - loss: 1.6202 - val_accuracy: 0.1989 - val_loss: 1.6206 - learning_rate: 5.0000e-05\n",
            "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 447ms/step - accuracy: 0.2028 - loss: 1.6206\n",
            "Test Loss: 1.6207, Test Accuracy: 0.20%\n"
          ]
        }
      ]
    }
  ]
}