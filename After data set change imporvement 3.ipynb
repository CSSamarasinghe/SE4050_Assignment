{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CSSamarasinghe/SE4050_Assignment/blob/IT21263194/After%20data%20set%20change%20imporvement%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwfpgjbDKHzW",
        "outputId": "76e588fe-dab2-431b-9e90-e897eaf6c7ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading amazon-reviews-for-sentianalysis-finegrained-csv, 654512809 bytes compressed\n",
            "[==================================================] 654512809 bytes downloaded\n",
            "Downloaded and uncompressed: amazon-reviews-for-sentianalysis-finegrained-csv\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'amazon-reviews-for-sentianalysis-finegrained-csv:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F2078107%2F3499094%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20241003%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20241003T164549Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D25a30a2ad799910d48f45f67793fb1d09f31560ae3773fa14870108018fa213261ae62aeb020867952a30d8094a21359c2546a6475193ac986bc2645a191d848c7223dbd5c68600aeb8f17d5a979be9750827328816b6743cf0d74ca79e5a28878f8606f4d3cd2176678d18b8a70a20bbf9be8037aa238d4b56bd1a0ba1a8d1fb4003bbd12f51fa6d438aaa8a7e8fa90a9291e62969f7df78c34dc6a71c76fb2f1c11e2964127136ec4d5554e9f1bdd7c530f8ddf987a08dd6cb91a849e4e98a2089149d22cb00ca687688c756071af4270e392fd79699c1f4a35ca16597523ca8bfe37fea11d5911465ea51d6eb17c1e3663da806763c8ef4e96e1f0a9ae9ab'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "train = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/train.csv')\n",
        "test = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/test.csv')\n",
        "\n",
        "# Check the column names\n",
        "print(\"Training data set\")\n",
        "print(train.columns)\n",
        "print(train.shape)\n",
        "\n",
        "print(\"Testing data set\")\n",
        "print(test.columns)\n",
        "print(test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i131J5_IKqy2",
        "outputId": "cc64685f-a7c6-4d5d-c012-882283c1243c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data set\n",
            "Index(['class_index', 'review_title', 'review_text'], dtype='object')\n",
            "(3000000, 3)\n",
            "Testing data set\n",
            "Index(['class_index', 'review_title', 'review_text'], dtype='object')\n",
            "(650000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Load the dataset\n",
        "train_data = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/train.csv')\n",
        "test_data = pd.read_csv('../input/amazon-reviews-for-sentianalysis-finegrained-csv/amazon_review_fine-grained_5_classes_csv/test.csv')\n",
        "\n",
        "# Reduce the dataset to a manageable size (e.g., taking the first 1000 samples)\n",
        "train_data = train_data.sample(n=60000, random_state=42)  # Randomly sample 1000 rows from the training data\n",
        "test_data = test_data.sample(n=25000, random_state=42)      # Randomly sample 200 rows from the test data\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(train_data['review_text'])\n",
        "\n",
        "# Convert text to sequences\n",
        "X_train = tokenizer.texts_to_sequences(train_data['review_text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_data['review_text'])\n",
        "\n",
        "# Directly take the numeric values from class_index\n",
        "Y_train = train_data['class_index'].values\n",
        "Y_test = test_data['class_index'].values\n",
        "\n",
        "# If needed, one-hot encode the labels for multi-class classification\n",
        "Y_train_reshaped = Y_train.reshape(-1, 1)\n",
        "Y_test_reshaped = Y_test.reshape(-1, 1)\n",
        "\n",
        "# Use sparse_output instead of sparse\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "Y_train_onehot = encoder.fit_transform(Y_train_reshaped)\n",
        "Y_test_onehot = encoder.transform(Y_test_reshaped)\n",
        "\n",
        "# Print the shapes to confirm\n",
        "print(f'X_train shape: {len(X_train)}, Y_train shape: {Y_train_onehot.shape}')\n",
        "print(f'X_test shape: {len(X_test)}, Y_test shape: {Y_test_onehot.shape}')\n",
        "\n",
        "# Pad the sequences and convert to tensors\n",
        "X_train_padded = pad_sequence([torch.tensor(seq) for seq in X_train], batch_first=True, padding_value=0)\n",
        "X_test_padded = pad_sequence([torch.tensor(seq) for seq in X_test], batch_first=True, padding_value=0)\n",
        "\n",
        "# Adjust the input size\n",
        "input_size = len(tokenizer.word_index) + 1  # Vocabulary size (plus 1 for padding)\n",
        "\n",
        "# Create TensorDataset and DataLoader\n",
        "train_dataset = TensorDataset(X_train_padded, torch.tensor(Y_train_onehot, dtype=torch.float32))\n",
        "test_dataset = TensorDataset(X_test_padded, torch.tensor(Y_test_onehot, dtype=torch.float32))\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU9X5ppvKxGj",
        "outputId": "9f73b2d1-b9da-4810-ea39-1f026ee3f865"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: 60000, Y_train shape: (60000, 5)\n",
            "X_test shape: 25000, Y_test shape: (25000, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNNWkvDOM_1N",
        "outputId": "33ef7a77-7e3e-4647-d41a-6a8df8de7709"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-05 00:12:39--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-10-05 00:12:39--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-10-05 00:12:39--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.5’\n",
            "\n",
            "glove.6B.zip.5      100%[===================>] 822.24M  5.09MB/s    in 2m 39s  \n",
            "\n",
            "2024-10-05 00:15:18 (5.18 MB/s) - ‘glove.6B.zip.5’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMtnRZ7JBX2E",
        "outputId": "52feed83-82e1-4bae-d569-9524e94b0c13"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of glove.6B.zip or\n",
            "        glove.6B.zip.zip, and cannot find glove.6B.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PqzLbBO9BaSv",
        "outputId": "ff459b8e-8c4a-4472-c92d-f6d73c5f0ce5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glove.6B.zip\tglove.6B.zip.2\tglove.6B.zip.4\tsample_data\n",
            "glove.6B.zip.1\tglove.6B.zip.3\tglove.6B.zip.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "def download_glove(glove_url, output_dir):\n",
        "    # Ensure output directory exists\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Download the GloVe file\n",
        "    response = requests.get(glove_url)\n",
        "    zip_file_path = os.path.join(output_dir, 'glove.6B.zip')\n",
        "\n",
        "    with open(zip_file_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(output_dir)\n",
        "\n",
        "    print(f\"GloVe embeddings downloaded and extracted to {output_dir}\")\n",
        "\n",
        "# URL for GloVe 300-dimensional embeddings\n",
        "glove_url = \"http://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "output_directory = \"./glove_embeddings\"\n",
        "\n",
        "download_glove(glove_url, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa1TBotIC8gv",
        "outputId": "a0bbf674-c3f1-4dbe-98c2-628f0cc0938e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GloVe embeddings downloaded and extracted to ./glove_embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_embeddings(file_path):\n",
        "    embeddings_index = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            coefs = np.array(values[1:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    return embeddings_index\n",
        "\n",
        "# Load GloVe embeddings\n",
        "glove_file_path = './glove_embeddings/glove.6B.300d.txt'  # Update this path if necessary\n",
        "glove_embeddings = load_glove_embeddings(glove_file_path)\n",
        "print(f'Loaded {len(glove_embeddings)} word vectors.')\n",
        "\n",
        "# Function to print embeddings for specific words\n",
        "def print_word_embeddings(words, embeddings):\n",
        "    for word in words:\n",
        "        if word in embeddings:\n",
        "            print(f\"Embedding for '{word}': {embeddings[word]}\")\n",
        "        else:\n",
        "            print(f\"'{word}' not found in GloVe model.\")\n",
        "\n",
        "# Specify words to print\n",
        "words_to_print = ['hello', 'world', 'example']  # Add any words you want to check\n",
        "print_word_embeddings(words_to_print, glove_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvFCdKZULZyv",
        "outputId": "172862f2-1c23-4b5c-f245-591f1f4fd38f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 word vectors.\n",
            "Embedding for 'hello': [-3.3712e-01 -2.1691e-01 -6.6365e-03 -4.1625e-01 -1.2555e+00 -2.8466e-02\n",
            " -7.2195e-01 -5.2887e-01  7.2085e-03  3.1997e-01  2.9425e-02 -1.3236e-02\n",
            "  4.3511e-01  2.5716e-01  3.8995e-01 -1.1968e-01  1.5035e-01  4.4762e-01\n",
            "  2.8407e-01  4.9339e-01  6.2826e-01  2.2888e-01 -4.0385e-01  2.7364e-02\n",
            "  7.3679e-03  1.3995e-01  2.3346e-01  6.8122e-02  4.8422e-01 -1.9578e-02\n",
            " -5.4751e-01 -5.4983e-01 -3.4091e-02  8.0017e-03 -4.3065e-01 -1.8969e-02\n",
            " -8.5670e-02 -8.1123e-01 -2.1080e-01  3.7784e-01 -3.5046e-01  1.3684e-01\n",
            " -5.5661e-01  1.6835e-01 -2.2952e-01 -1.6184e-01  6.7345e-01 -4.6597e-01\n",
            " -3.1834e-02 -2.6037e-01 -1.7797e-01  1.9436e-02  1.0727e-01  6.6534e-01\n",
            " -3.4836e-01  4.7833e-02  1.6440e-01  1.4088e-01  1.9204e-01 -3.5009e-01\n",
            "  2.6236e-01  1.7626e-01 -3.1367e-01  1.1709e-01  2.0378e-01  6.1775e-01\n",
            "  4.9075e-01 -7.5210e-02 -1.1815e-01  1.8685e-01  4.0679e-01  2.8319e-01\n",
            " -1.6290e-01  3.8388e-02  4.3794e-01  8.8224e-02  5.9046e-01 -5.3515e-02\n",
            "  3.8819e-02  1.8202e-01 -2.7599e-01  3.9474e-01 -2.0499e-01  1.7411e-01\n",
            "  1.0315e-01  2.5117e-01 -3.6542e-01  3.6528e-01  2.2448e-01 -9.7551e-01\n",
            "  9.4505e-02 -1.7859e-01 -3.0688e-01 -5.8633e-01 -1.8526e-01  3.9565e-02\n",
            " -4.2309e-01 -1.5715e-01  2.0401e-01  1.6906e-01  3.4465e-01 -4.2262e-01\n",
            "  1.9553e-01  5.9454e-01 -3.0531e-01 -1.0633e-01 -1.9055e-01 -5.8544e-01\n",
            "  2.1357e-01  3.8414e-01  9.1499e-02  3.8353e-01  2.9075e-01  2.4519e-02\n",
            "  2.8440e-01  6.3715e-02 -1.5483e-01  4.0031e-01  3.1543e-01 -3.7128e-02\n",
            "  6.3363e-02 -2.7090e-01  2.5160e-01  4.7105e-01  4.9556e-01 -3.6401e-01\n",
            "  1.0370e-01  4.6076e-02  1.6565e-01 -2.9024e-01 -6.6949e-02 -3.0881e-01\n",
            "  4.8263e-01  3.0972e-01 -1.1145e-01 -1.0329e-01  2.8585e-02 -1.3579e-01\n",
            "  5.2924e-01 -1.4077e-01  9.1763e-02  1.3127e-01 -2.0944e-01  2.2327e-02\n",
            " -7.7692e-02  7.7934e-02 -3.3067e-02  1.1680e-01  3.2029e-01  3.7749e-01\n",
            " -7.5679e-01 -1.5944e-01  1.4964e-01  4.2253e-01  2.8136e-03  2.1328e-01\n",
            "  8.6776e-02 -5.2704e-02 -4.0859e-01 -1.1774e-01  9.0621e-02 -2.3794e-01\n",
            " -1.8326e-01  1.3115e-01 -5.5949e-01  9.2071e-02 -3.9504e-02  1.3334e-01\n",
            "  4.9632e-01  2.8733e-01 -1.8544e-01  2.4618e-02 -4.2826e-01  7.4148e-02\n",
            "  7.6584e-04  2.3950e-01  2.2615e-01  5.5166e-02 -7.5096e-02 -2.2308e-01\n",
            "  2.3775e-01 -4.5455e-01  2.6564e-01 -1.5137e-01 -2.4146e-01 -2.4736e-01\n",
            "  5.5214e-01  2.6819e-01  4.8831e-01 -1.3423e-01 -1.5918e-01  3.7606e-01\n",
            " -1.9834e-01  1.6699e-01 -1.5368e-01  2.4561e-01 -9.2506e-02 -3.0257e-01\n",
            " -2.9493e-01 -7.4917e-01  1.0567e+00  3.7971e-01  6.9314e-01 -3.1672e-02\n",
            "  2.1588e-01 -4.0739e-01 -1.5264e-01  3.2296e-01 -1.2999e-01 -5.0129e-01\n",
            " -4.4231e-01  1.6904e-02 -1.1459e-02  7.2293e-03  1.1026e-01  2.1568e-01\n",
            " -3.2373e-01 -3.7292e-01 -9.2456e-03 -2.6769e-01  3.9066e-01  3.5742e-01\n",
            " -6.0632e-02  6.7966e-02  3.3830e-01  6.5747e-02  1.5794e-01  4.7155e-02\n",
            "  2.3682e-01 -9.1370e-02  6.4649e-01 -2.5491e-01 -6.7940e-01 -6.9752e-01\n",
            " -1.0145e-01 -3.6255e-01  3.6967e-01 -4.1295e-01  8.2724e-02 -3.5053e-01\n",
            " -1.7564e-01  8.5095e-02 -5.7724e-01  5.0252e-01  5.2180e-01  5.7327e-02\n",
            " -7.9754e-01 -3.7770e-01  7.8149e-01  2.4597e-01  6.0672e-01 -2.0082e-01\n",
            " -3.8792e-01  4.1295e-01 -1.6143e-01  1.0427e-02  4.3197e-01  4.6297e-03\n",
            "  2.1185e-01 -2.6606e-01 -5.8740e-02 -5.1003e-01  2.8524e-01  1.3627e-02\n",
            " -2.7346e-01  6.1848e-02 -5.7901e-01 -5.1136e-01  3.6382e-01  3.5144e-01\n",
            " -1.6501e-01 -4.6041e-01 -6.4742e-02 -6.8310e-01 -4.7427e-02  1.5861e-01\n",
            " -4.7288e-01  3.3968e-01  1.2092e-03  1.6018e-01 -5.8024e-01  1.4556e-01\n",
            " -9.1317e-01 -3.7592e-01 -3.2950e-01  5.3465e-01  1.8224e-01 -5.2265e-01\n",
            " -2.6209e-01 -4.2458e-01 -1.8034e-01  9.9502e-02 -1.5114e-01 -6.6731e-01\n",
            "  2.4483e-01 -5.6630e-01  3.3843e-01  4.0558e-01  1.8073e-01  6.4250e-01]\n",
            "Embedding for 'world': [-0.25831    0.43644   -0.1138    -0.5259     0.20213    0.95247\n",
            " -0.58764   -0.047001  -0.053704  -1.744      0.99583    0.063464\n",
            " -0.093147  -0.26441   -0.28676   -0.52357   -0.17867    0.18171\n",
            " -0.71696   -0.13301    0.42476    0.42044    0.3775     0.082431\n",
            "  0.13154   -0.10151   -0.11898    0.029509  -0.39635    0.26516\n",
            " -0.55091    0.23805   -0.018748  -0.039944  -1.1972     0.13567\n",
            "  0.09371   -0.60134    0.12887    0.34876   -0.25588   -0.33466\n",
            "  0.069678   0.5429     0.25246    0.17249    0.099885   0.099456\n",
            " -0.01592    0.2617     0.36155   -0.12417    0.27516    0.037434\n",
            " -0.075003   0.61096    0.05261    0.017307   0.12576   -0.11952\n",
            " -0.49077    0.026711  -0.27187   -0.15268   -0.22147    0.18131\n",
            " -0.045344   0.76151    0.17489   -0.44112    0.027347   0.42676\n",
            " -0.0069618 -0.60233   -0.016613   0.18418    0.021843  -0.34176\n",
            " -0.55154    0.35013    0.42137   -0.26789   -0.18035    0.053171\n",
            "  0.14083    0.29055    0.152      0.01439    0.38663    0.030334\n",
            " -0.14704   -0.036147   0.27349   -0.21787    0.19873    0.12488\n",
            " -0.049808   0.41397   -0.14756   -0.41982    0.31144    0.043027\n",
            "  0.13019   -0.1066     0.19274   -0.035078   0.29942   -0.11902\n",
            " -0.44143    0.30043   -0.14127    0.025526  -0.13042   -0.14063\n",
            "  0.073307   0.073228   0.24368    0.52377    0.049154  -0.11187\n",
            " -0.52428    0.010288   0.16688   -0.0030069  0.22922    0.66468\n",
            " -0.36986    0.095543  -0.26651   -0.57594    0.16846    0.26121\n",
            "  0.2439    -0.45329   -0.52662   -0.011254  -0.017331  -0.30143\n",
            " -0.52018   -0.11831   -0.28169    0.19066    0.13315    0.41312\n",
            "  0.23883   -0.10314    0.62913   -0.091733  -0.28854    0.16397\n",
            "  0.44321   -0.60102   -0.1172    -0.056136   0.60052   -0.054252\n",
            " -0.55247   -0.11621    0.068191  -0.45674    0.032338   0.051022\n",
            "  0.22195   -0.16676    0.033077  -0.15372    0.20906    0.3812\n",
            " -0.13643   -0.27834    0.06265    0.27672   -0.56545   -0.2464\n",
            "  0.29436    0.16149    0.77578   -0.14377   -0.11131    1.1964\n",
            " -0.067608   0.46178    0.69352   -0.31582    0.64509    0.072062\n",
            " -0.50423    0.27635   -0.13346    0.01853    0.25858   -0.15529\n",
            "  0.12289   -0.34966   -0.48943    0.11885    0.16122   -0.25976\n",
            "  0.15021    0.11598    1.8908     0.23723    0.14245   -0.32139\n",
            " -0.16806    0.26471   -0.27449   -0.010458   0.044555  -0.21198\n",
            " -0.17074    0.046153  -0.21848   -0.26406   -0.14896    0.60799\n",
            " -0.56122   -0.36819   -0.15405    0.16577    0.33116   -0.48585\n",
            " -0.24098   -0.029899   0.17621    0.045554  -0.0063902  0.36114\n",
            " -0.49063   -0.041788   0.080747   0.061024   0.39921    0.01599\n",
            "  0.15847    0.0091571  0.38123    0.20664   -0.39652   -0.26899\n",
            " -0.77855    0.01754    0.242      0.17483   -0.62412   -0.093079\n",
            " -0.29577   -0.0076823 -0.034885   0.25996    0.31075   -0.36391\n",
            "  0.21322    0.19122    0.0075838 -0.43858    0.12941   -0.73911\n",
            " -0.032862  -0.038055   0.19221   -0.43609   -0.2739    -0.15047\n",
            "  0.11068    0.34753   -0.61807   -0.0093843  0.19075    0.32107\n",
            " -0.13255    0.36662    0.75575    0.94367    0.10123   -0.13744\n",
            " -2.0208     0.069843   0.83247   -0.22516   -0.52209   -0.60384\n",
            " -0.38571    0.19936    0.519     -0.28605   -0.39936    0.12247\n",
            " -0.77574   -0.078337  -0.35379   -0.40318   -0.84683    0.089815\n",
            "  0.21117    0.45053    0.35026    0.081697  -0.0044191 -0.14102  ]\n",
            "Embedding for 'example': [-0.20297    0.010222   0.063166  -0.23259    0.063991   0.13313\n",
            " -0.12547   -0.12964   -0.2553    -1.7295     0.091936   0.021622\n",
            " -0.31398    0.011587   0.2258    -0.15155   -0.081282  -0.22776\n",
            "  0.0030483 -0.2752     0.010934   0.05136    0.22624    0.43196\n",
            "  0.012415  -0.25724   -0.093916  -0.10665   -0.071235  -0.10342\n",
            " -0.098095   0.48373   -0.67475   -0.10943   -0.50882   -0.05844\n",
            "  0.15398   -0.17816   -0.39477    0.066937   0.041519  -0.085288\n",
            " -0.21029    0.2462    -0.37551   -0.22549   -0.082648   0.37232\n",
            " -0.14809    0.0786     0.11278   -0.10294    0.37601    0.15898\n",
            " -0.17571   -0.063246  -0.08103   -0.10676    0.094943   0.28391\n",
            "  0.35478    0.10635    0.51442   -0.10787   -0.14091   -0.087904\n",
            " -0.15251    0.1862     0.14508    0.28751   -0.36265   -0.054447\n",
            "  0.083526   0.015638  -0.38204   -0.10227   -0.022685   0.24539\n",
            " -0.14744   -0.13984   -0.29816   -0.079155  -0.042634  -0.20823\n",
            "  0.32099    0.11908   -0.10361    0.36009   -0.097773  -0.095832\n",
            " -0.22441    0.40845   -0.24683    0.004935   0.26121   -0.30236\n",
            " -0.055492  -0.060934   0.32264   -0.32745   -0.049976   0.17791\n",
            "  0.18079   -0.019663  -0.13955   -0.045405  -0.097046   0.1756\n",
            " -0.076284   0.18398    0.22837   -0.16035    0.12625    0.24772\n",
            "  0.1628     0.049716   0.17262    0.23504   -0.44126   -0.013569\n",
            "  0.13762    0.096705   0.17      -0.075387   0.005149   0.11469\n",
            " -0.042954   0.33368    0.02252   -0.032179   0.0066799  0.23217\n",
            "  0.009376   0.043549  -0.13701   -0.18404   -0.14659   -0.14246\n",
            " -0.012047   0.30807   -0.021983   0.035699  -0.16054   -0.031032\n",
            "  0.010459  -0.26445    0.12911    0.11047    0.098834   0.18523\n",
            "  0.021405  -0.29562    0.19779   -0.43833    0.095736   0.0025402\n",
            "  0.019167   0.3281     0.1569    -0.15273    0.25866    0.045144\n",
            " -0.011598   0.025048  -0.050747   0.3272    -0.10882    0.051519\n",
            "  0.12533    0.17068    0.18793   -0.14652   -0.58708    0.51169\n",
            "  0.011755  -0.19662   -0.10641   -0.099865  -0.16226   -0.020669\n",
            " -0.15008   -0.051639  -0.056518  -0.17516   -0.0032804 -0.24018\n",
            " -0.31748    0.12491    0.11242    0.15702   -0.11319    0.24022\n",
            "  0.44465    0.24607   -0.14982   -0.13391    0.038901  -0.24031\n",
            " -0.34008   -0.35809    0.4656    -0.18441    0.15712   -0.15752\n",
            "  0.15639    0.19543   -0.19702    0.19786   -0.095468   0.0629\n",
            " -0.093645  -0.039319   0.31595   -0.40324   -0.086324   0.2483\n",
            "  0.011525   0.019539  -0.1726    -0.16493   -0.17982    0.3295\n",
            "  0.018484   0.046018   0.05142   -0.015903   0.15328    0.11484\n",
            " -0.051323  -0.019173  -0.3888     0.30587   -0.077293   0.18696\n",
            "  0.0045978  0.22228    0.11091    0.021999  -0.13843    0.02481\n",
            " -0.29732   -0.01645    0.1462    -0.030213  -0.64828    0.002871\n",
            " -0.078324  -0.22465   -0.10246    0.065213   0.028865   0.16099\n",
            " -0.25852   -0.19846    0.095005   0.14001   -0.19232    0.058352\n",
            "  0.019701  -0.18324   -0.054571   0.0021274  0.13715   -0.18022\n",
            "  0.095572  -0.13542   -0.22389    0.14396    0.36295    0.17062\n",
            "  0.21415    0.049573   0.017449  -0.088432  -0.2731     0.14752\n",
            " -1.8151    -0.10578    0.40674    0.049543  -0.212     -0.26251\n",
            "  0.017655  -0.059089  -0.022608   0.28688   -0.075669  -0.05715\n",
            "  0.19422    0.044419  -0.030983   0.0087946 -0.10776    0.16817\n",
            "  0.34083   -0.13625    0.10977   -0.19285    0.19969    0.023081 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare embedding matrix\n",
        "embedding_dim = 300  # Dimension of GloVe vectors\n",
        "embedding_matrix = np.zeros((input_size, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < input_size:  # Ensure we don't exceed the matrix size\n",
        "        embedding_vector = glove_embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "czwgL40cLuwf"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size, embedding_matrix, dropout_rate=0.5):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(torch.from_numpy(embedding_matrix))  # Load pre-trained weights\n",
        "\n",
        "        self.lstm1 = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        self.layer_norm1 = nn.LayerNorm(hidden_size)  # Layer normalization after lstm1\n",
        "        self.layer_norm2 = nn.LayerNorm(hidden_size)  # Layer normalization after lstm2\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)  # Dropout layer\n",
        "        self.fc1 = nn.Linear(hidden_size, hidden_size)  # Additional fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)  # Output layer\n",
        "\n",
        "        # Leaky ReLU with a negative slope of 0.01\n",
        "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        x, _ = self.lstm1(x)\n",
        "        x = self.layer_norm1(x)  # Apply layer normalization after the first LSTM\n",
        "        x = self.leaky_relu(x)    # Use Leaky ReLU activation after lstm1\n",
        "\n",
        "        x, _ = self.lstm2(x)\n",
        "        x = self.layer_norm2(x)  # Apply layer normalization after the second LSTM\n",
        "\n",
        "        x = self.dropout(x[:, -1, :])  # Apply dropout to the last time step output\n",
        "        x = self.fc1(x)                 # First fully connected layer\n",
        "        x = F.relu(x)                   # ReLU activation function for fully connected layer\n",
        "        x = self.fc2(x)                 # Output layer\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "PfD5UeJ3Kzpl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assuming you have already defined and loaded your GloVe embeddings and created the embedding matrix\n",
        "# Load GloVe embeddings and create embedding_matrix (as discussed previously)\n",
        "\n",
        "# Set hyperparameters\n",
        "num_epochs = 5  # Number of training epochs\n",
        "embedding_dim = 300  # Dimension of GloVe embeddings (adjust based on your GloVe file)\n",
        "hidden_size = 64  # Number of LSTM hidden units\n",
        "output_size = len(set(Y_train))  # Number of classes based on your training labels\n",
        "learning_rate = 0.001  # Learning rate for the optimizer\n",
        "\n",
        "# Ensure input_size is defined based on your tokenizer\n",
        "input_size = len(tokenizer.word_index) + 1  # Vocabulary size (plus padding)\n",
        "\n",
        "# Initialize the model **before** defining the optimizer\n",
        "model = LSTMModel(vocab_size=input_size,\n",
        "                  embedding_dim=embedding_dim,\n",
        "                  hidden_size=hidden_size,\n",
        "                  output_size=output_size,\n",
        "                  embedding_matrix=embedding_matrix,  # Ensure this is defined\n",
        "                  dropout_rate=0.5)  # You can adjust the dropout rate as needed\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "5-EFfP6AK199"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set the model to training mode\n",
        "    epoch_loss = 0  # Initialize loss for the epoch\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "\n",
        "        inputs = inputs.long()  # Ensure inputs are long type for embedding\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "\n",
        "        # Compute loss (labels should be LongTensor of class indices)\n",
        "        loss = criterion(outputs, labels.argmax(dim=1))  # Use argmax to get class indices from one-hot labels\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimization step\n",
        "\n",
        "        epoch_loss += loss.item()  # Accumulate loss for this batch\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)  # Calculate average loss for the epoch\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDzhsCy1Ptcd",
        "outputId": "828056f6-8f6c-4ecc-cbb6-80b53cf92e24"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 1.6128\n",
            "Epoch [2/5], Loss: 1.6100\n",
            "Epoch [3/5], Loss: 1.6099\n",
            "Epoch [4/5], Loss: 1.6096\n",
            "Epoch [5/5], Loss: 1.6096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    # Disable gradient calculation for evaluation\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs = inputs.long()  # Ensure inputs are long type for embedding\n",
        "            inputs, labels = inputs.to('cuda' if torch.cuda.is_available() else 'cpu'), labels.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "            # Forward pass through the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get predicted class indices\n",
        "            _, predicted_classes = torch.max(outputs.data, 1)\n",
        "\n",
        "            # Convert one-hot encoded labels back to class indices for comparison\n",
        "            true_classes = torch.argmax(labels, dim=1)\n",
        "\n",
        "            total_samples += labels.size(0)\n",
        "            correct_predictions += (predicted_classes == true_classes).sum().item()\n",
        "\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = correct_predictions / total_samples * 100.0\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "# Example usage: Evaluate on the test set\n",
        "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HK19DwhABzm",
        "outputId": "47b16256-8dcd-453f-9f4c-57944b1a59dd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.6095, Test Accuracy: 19.85%\n"
          ]
        }
      ]
    }
  ]
}