# -*- coding: utf-8 -*-
"""ANN Training01-Additional layers

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WMCpCHo8Y1sXS7rysWq6f_pbFnMRL4Yt
"""

import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Dense, Dropout, Flatten

# Load the IMDb dataset
(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=10000)

# Pad sequences to ensure all input data is of the same length
max_length = 200
X_train_padded = pad_sequences(X_train, maxlen=max_length)
X_test_padded = pad_sequences(X_test, maxlen=max_length)

# Check the number of samples (length of the dataset)
print("Number of training samples:", len(X_train))
print("Number of test samples:", len(X_test))

# Check the length of the first review (before padding)
print("Length of first training review:", len(X_train[0]))

# Build the ANN model
model = Sequential()

# Embedding layer
model.add(Embedding(input_dim=10000, output_dim=128))

# Flatten layer: Flattens the output from the Embedding layer before passing it to the Dense layers
model.add(Flatten())

# Fully connected hidden layer
model.add(Dense(256, activation='relu'))  # Increased neurons
model.add(Dropout(0.5))  # Dropout to prevent overfitting

# Add an additional Dense layer with 128 neurons
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))

# Add another Dense layer with 64 neurons
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Add another Dense layer with 32 neurons
model.add(Dense(32, activation='relu'))  # New layer with 32 neurons
model.add(Dropout(0.5))

# Increase neurons in Dense layers
model.add(Dense(512, activation='relu'))
model.add(Dropout(0.5))

# Output layer for binary classification
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Print the model summary
model.summary()

from tensorflow.keras.layers import BatchNormalization

# Example Dense layer with Batch Normalization and Dropout
model.add(Dense(256, activation='relu'))  # Fully connected hidden layer
model.add(BatchNormalization())  # Add Batch Normalization here
model.add(Dropout(0.5))  # Add Dropout after Batch Normalization

model.add(Dense(128, activation='relu'))
model.add(BatchNormalization())  # Another Batch Normalization
model.add(Dropout(0.5))

# Train the model
history = model.fit(X_train_padded, y_train, epochs=15, batch_size=64, validation_split=0.2)

# Evaluate the model on test data
test_loss, test_acc = model.evaluate(X_test_padded, y_test)

print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_acc}')

"""Predicting Sentiment for new reviews"""

# Predict sentiment on a new review
def predict_sentiment(review):
    # Convert the review to a sequence of integers and pad
    review_seq = imdb.get_word_index()
    tokens = [review_seq.get(word, 2) for word in review.lower().split()]  # 2 is the default for unknown words
    padded_tokens = pad_sequences([tokens], maxlen=max_length)

    # Predict the sentiment (1 = positive, 0 = negative)
    prediction = model.predict(padded_tokens)[0][0]
    return 'Positive' if prediction > 0.5 else 'Negative'

def predict_sentiment(review):
    # Convert the review to a sequence of integers and pad
    review_seq = imdb.get_word_index()
    tokens = [review_seq.get(word, 2) for word in review.lower().split()]  # 2 is the default for unknown words
    padded_tokens = pad_sequences([tokens], maxlen=max_length)

    # Predict the sentiment (1 = positive, 0 = negative)
    prediction = model.predict(padded_tokens)[0][0]

    # Define thresholds for classification
    if prediction > 0.6:
        return 'Positive'
    elif prediction < 0.4:
        return 'Negative'
    else:
        return 'Neutral'  # Prediction is > 0.4 and < 0.6

# Example usage
print(predict_sentiment("wow I really liked the movie!"))  # Expected: Positive
print(predict_sentiment("Watching this moive was like enduring a bad joke that refuses to end. The plot was a chaotic mess, with barely any coherent direction or purpose. It felt like the filmmakers tried to cram in every clichÃ© possible, and the result was an uninspired jumble of tropes we've all seen a thousand times before."))       # Expected: Negative
#print(predict_sentiment("it was fine."))            # Expected: Neutral

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_padded, y_test)

print(f"Test Accuracy: {test_accuracy:.4f}")

"""***IT21227622***"""